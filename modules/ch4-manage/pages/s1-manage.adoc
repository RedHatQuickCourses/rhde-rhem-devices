:time_estimate: 14

= Manage Edge Devices with Red Hat Edge Manager

_Estimated reading time: *{time_estimate} minutes*._

Objective::

Configure and update managed devices using RHEM.

WARNING: Work in progress

////

How to inject system configurations, including sensitive configurations (secrets), on a device after or during enrollment.

* Discussion of what to configure at image build time, at device provisioning time (from kickstart or cloud-init) and at day-2.

* Introduce the concept of onboarding as adding configurations which make a device able to function, for example secrets to connect to other edge or corporate network services, which are not recommended to bake at build or installation time.

* Updating the SO/image in a device, updating settings, onboarding, or repurposing a device to serve a different purpose are all the same in the bootc/RHEM world. Itâ€™s about the intent, but use the same YAML/APIs/commands in RHEM.

* Mention common issues with updating or switching bootc images and refer to the image mode quick course series.

////

As we already learned, Red Hat Edge Manager (RHEM) manages three aspects of a device configuration:

* Its operating system image, as a bootc container image.

* Operating system and application configuration files, in the `/etc` directory and other locations.

* Workloads running as containerized applications.

In this course, we focus on the first two, and we teach about managing workloads or applications with RHEM in the next course.

Before presenting RHEM device templates, let's discuss about when it is appropriate to use RHEM to configure a device.

== Operating system configuration at image build time, day-1, and day2.

With image mode for RHEL, it is expected that most configurations configurations are set by a bootc container image, which would be configurations set at day-0, before provisioning a system, during image build time.
These configurations will be the same for all devices using the image.

Having configurations applied to a system image, so they are applied together with installing the operating system, instead of applied only after installing the operating system, ensures devices have a know starting state and reduces the risk of accidental drift.

While it is possible to override configurations set by a bootc container image, because the `/etc` and `/var` directories are read-write, these changes should be kept to a minimum and done only on purpose.
Modern Linux applications and services are expected to take their primary configurations from the `/usr/lib` directory, which is read-only on image mode systems, and use drop directories such as `/etc/something.d` for configuration snippets that add to or override their primary configurations.

There is always some configuration settings that must vary between different devices, or between different edge sites, and writable configuation files in the `/etc` directory should be reserved such exceptions.

There is also the matter of secrets management: it is considered a very bad security practice to embed authentication credentials such as passwords, tokens, and certificates in system images.
For example, you there should not exist any predefined user with a well-known password which could be used to access production systems, including edge devices.

Per-device and per-site configurations, and also secrets, can be applied to devices at day-1, that is, at device provisioning time (or installation time), or at day-2, which really means as a regular day-to-day management operation.

Most operating system installation and provisioning methods offer some way of adding configuration files and running configuration scripts.
The RHEL installer, called Anaconda, provides the *kickstart* feature, which can set all settings the installer would otherwize prompt for during an interactive installation, and also run arbitrary scripts.
It is a common practice to embed custom kickstart files in RHEL installation media, or in network boot servers, with both package and image modes.

Most hypervisors and cloud provides nowadays support the *cloud-init* configuration mechanism, which has similar capacities to kickstart, but with its own syntax.

By using kickstart or cloud-init scripts, you can perform bootstrapping configurations such as setting a SSH key to access the newly provisioned machines.
Having initial users and access credentials set this way makes them easier and safer to change at day-2, because they are not set by read-only files in the system image, and avoids the risk of reverting to whatever was set by the image.
But it also can be considered a risk of leak, when they are stored in installation media.

Per-device and per-site configurations, if they are expected to be set only once, at provisioning time, and never (or rarely) changed on day-2, are good candidates for kickstart and cloud-init.

Anything that you are expected to change regularly, after provisioning a device, is a good candidate for traditional operating system management tools, such as Red Hat Ansible Automation Platform, and also for edge management tools, such as RHEM.

Some organizations prefer to use day-2 tools to perform first-time configuration of new devices, that is to perform day-1 configuration on those devices.
Such organizations could provision devices using a very generic image, use day-1 just to set credentials for a management tool, and them let the management tool perform the remaining day-1 configurations.

== Oboarding devices

By the previous discussion, you know that RHEM adds configurations over whatever was already set by a bootc container image and by a provisioning mechanism, such as kickstart.
RHEM is not supposed to take over and control all configurations of a device.
You can still perform ad-hoc, manual configuration of anything that is not managed by RHEM.
And you can configure some settings using a different day-2 configuration tool, such as Ansible.

Of course, having a device configured by multiple different tools can be a challenge to manage, but you may find a scenario where it is easier, or more secure, doing it with a different tool, or using RHEM as one of the tools in a larger workflow.

For example, by onboarding an edge device most organizations mean more than just performing its first-time (day-1) configuration.
If your devices run applications which require access to other devices, cloud resources, or corporate systems, they need access credentials to all that.
Providing the required credentials securely is not a trivial task and there are specialized tools for secrets management, for example Hashicorp Vault.
You could configure everything except secrets using RHEM, and let Vault configure secrets for the device.

As another example, if your organization sets network access from devices based on their MAC address, you may need to configure networking gear as part of your device onboarding process, to set the MAC address to the desired fixed IP address, and to set the ethernet port of the device to the desired VLAN.

If you must implement a more sophisticated onboarding workflow, you may need a tool capable of orchestrating RHEM together with other management tools.
Red Hat Ansible Automation Platform is great for such tasks.

== Drift detection and reconcilliation

So RHEM can manage operating and application configuration files, but does not need to manage all of them in any given device.
If a file is not set by RHEM configurations in a device or fleet template, they can be set by any other means.

You cannot use RHEM to forcibly lock down all configuration files in a device and prevent users or tools to perform unintended changes to those files.
But, if a file is set by RHEM configurations, RHEM will detect changes to it (configuration drift) and fix it, reverting the file to whatever contents are set by RHEM.

While it is not practical to add too many configuration files to a device template "just in case", because such configurations would be huge (just check how many files exist in the `/etc` directory of any Linux system!), RHEM provides you with a reliable recovery path from unintended changes: reinstall the device, as if were is a newly provisioned device, and let RHEM apply the expected configurations to it.
This way, you can restore any device to a known good configuration state, considering all of day-0, day-1, and day-2.

Now that you have some ideas about what to configure and when to configure something in a device using RHEM, let's focus on the mechanics of RHEM configurations.

== RHEM configuration types

If a managed device is not a member of a fleet, its device API resource may contain a list of configuration items, a list of applications, and a reference to an operating system image.

Each configuration item has a name, which is supposedly a descriptive name, and a type, which specifies the configuration provider responsible for retrieving the contents of the configuration files.

With RHEM on RHEL, you would normally use these three configuration providers:

inline::
Inline configurations are stored in the device API resource itself, using the ignition format.
If these configuration files are text files, they are just YAML multiline string values.

HTTP::
Configurations are retrieved from a web server or anything that can respond to HTTP requests, like an S3-compatible object storage.
This alternative is commonly used to simplify integration between management tools, for example when RHEM runs integrated with Red Hat Advanced Cluster Manager, so the other management tool provides configuration files for devices managed by RHEM.

Git::
Configurations are retrieved from a git repository.
This enables RHEM to work as GitOps for the operating system.
It also enables easy testing and troubleshooting of configurations, because you do not need to extract and embed configuration files in a RHEM API resource: the configuration files are just regular files in a Git tree.

Notice that a single device can contain multiple configuration items, using different providers, or even using the same provider.
And each configuration item can contain multiple configuration files.
This enables logical grouping of configuration files by purpose, and enables a degree of separation of concerns.

For example, a RHEM administrator may opt for configuring some files as inline configurations, and keep control over them, while configuring other files from a Git repository, and delegate those files to non-RHEM administrators with access to that repository.

Managed devices never connect directly to external configuration providers.
For example, managed devices do not connect directly to Git servers, so they do not require access credentials to a Git repository.
The RHEM server connects to configuration provides and grab all required files, and a RHEM agent downloads all of them from its RHEM server.

NOTE: In this course, we focus on the inline configuration provider, so we can learn the basics of RHEM.
In the next course we focus on the Git configuration provider, and use fleets to demonstrate an effective GitOps workflow.

== Operating system image changes

There is not a lot to say about managing operating system images with RHEM: a device API resource contains the name of a bootc container image.
If that field changes, the RHEM agent performs a `bootc switch` operation and reboots the device.

The new system image does not need to be related, in any way, to the previous system image.
It could provide a different RHEL release, a different set of system services, and come from a different container registry.
RHEM and bootc do not care: it's just another system image.

That enables you to implement effective repurposing workflows, switching a device to a diferent system image, with different configurations, and running different applications.

During initial device enrollment, the operating system image field is left empty: RHEM does not record the image which the device was booted (or provisioned) with.
The system image is considered not managed by RHEM, and changeable manually by using the `bootc` command on the device, until a RHEM user explicit sets the value of the operating system image in the device template.

== Updating managed devices

Updating a device with RHEM may not mean what you think an update is.
Most IT professionals intuitively think about device updates as operating system updates, or patching.

To RHEM, any change to a device template -- including a change of configurations or applications -- is an update.
It does not need to be an update of the operating system, that is, it does not meed to make a manage device switch to an updated operating system image.

RHEM treats all of the device template as a unit, during a device reconcilliation, so if you change all three aspects of a device template, at the same time, RHEM applies all changes to configurations, system image, and applications, at one.
If any of them fails, for whatever reason, the entire update operation, or reconcilliation, failed and the device status is set to `Out-of-Date`.

If you check the device status details, you can get information about what the device failed to update, and why.
For example, a device may fail to download a bootc container image because it lacks credentials to access a container image registry.

RHEM has no concept of retrying an update.
If there's a failure during device reconcilliation, you must change the device template, thus forcing another update operation.
RHEM assumes that updates must work all the times, else there is an issue with the device template itself, which must be fixed.

But how RHEM prevents intermitent errors?
The RHEL agent first downloads all artifacts it needs: bootc container images, configuration files, and application containers.
Only after all of them are completely downloaded and verified, it starts applying them to the device itself.

The agent may take time to download all artifacts, especially under intermitent connections, and the device status in RHEM states what the agent is still downloading artifacts.

If an update fails because of a condition internal to the device, for example low disk space, this is still considered a device template issue: it should not force into devices more files than they fit.

== Shell sessions on managed devices

RHEM enables its users to get a shell on a managed devices and run commands as the root user.
It uses the network connection the agent estabilishes with the RHEM server, so a device does not need to have an addressable IP address, like you would need for SSH connections.

You even could opt to not run an SSH daemon at all on managed devices and do all remote access using RHEM.

The `flightctl console` command or the menu:Terminal[] tab in the menu:Device details[] page of the web UI provide with RHEM shell sessions.

== Permissions and roles

We already learned that you must be a RHEM administrator to approve device enrollment requests, but you do not need such privileges to manage devices.

RHEM recognizes three access roles: administrator, operator, and viewer.
Users with either the administrator or the operator role can manage devices, that is, change their device templates and start shell sessions.
Users with the viewer role, as expected, cannot make such changes.

NOTE: In this course we do not teach the use of organizations, which enable multi-tenancy withing a RHEL server.
RHEM users might be assigned different roles in different organizations, and this is set by group membership using the PAM issuer.

== What's next

The next two activities exercise device configuration and operating system updates with RHEM, and the next chapter teaches how to exclude a device from RHEM management.
